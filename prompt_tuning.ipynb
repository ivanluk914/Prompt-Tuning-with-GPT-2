{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt Tuning with `GPT-2`\n",
    "\n",
    "This project aims to explore prompt tuning for improving the task-specific performance of a causal language model (CLM), specifically `GPT-2`. Prompt tuning is a parameter-efficient fine-tuning method that optimizes a small set of virtual tokens prepended to the input sequence, enabling task adaptation without updating the entire model’s parameters. This approach is particularly useful for reducing computational overhead while leveraging the capabilities of pre-trained large language models.\n",
    "\n",
    "### Background\n",
    "\n",
    "Traditional fine-tuning methods require updating all model parameters, making them computationally expensive and less feasible for resource-constrained environments. In contrast, prompt tuning introduces and trains a small number of task-specific parameters, allowing efficient adaptation for various tasks while preserving the pre-trained model’s general knowledge.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset used for this project is derived from the Hugging Face repository `fka/awesome-chatgpt-prompts`, a collection of task-oriented prompts designed to guide language models. This dataset allows evaluation on a variety of tasks, such as text completion, translation, or instruction following.\n",
    "\t•\tTrain Size: 100 samples\n",
    "\t•\tFormat: JSON with fields for prompt-response pairs.\n",
    "\n",
    "### Methods\n",
    "\n",
    "1.\tModel Configuration:\n",
    "\n",
    "    •\tPre-trained `GPT-2` model\n",
    "\n",
    "    •\tPrompt tuning applied using the PEFT (Parameter-Efficient Fine-Tuning) library.\n",
    "\n",
    "2.\tPrompt Tuning Setup:\n",
    "\n",
    "    •\tNumber of virtual tokens: Initially set to 5.\n",
    "\n",
    "    •\tInitialization: Random embeddings.\n",
    "\n",
    "    •\tTask type: Causal Language Modeling.\n",
    "\n",
    "    •\tOptimization: AdamW with a learning rate of 0.005.\n",
    "\n",
    "\n",
    "3.\tTraining Process:\n",
    "\n",
    "    •\tInitial configuration: `epochs=5`, `virtual_tokens=5`.\n",
    "\n",
    "    •\tResults were suboptimal, suggesting the model required more iterations for convergence.\n",
    "\n",
    "    •\tUpdated configuration: `epochs=10`, `virtual_tokens=5`.\n",
    "\n",
    "    •\tThis resulted in significantly improved performance, with training time still being very acceptable.\n",
    "\n",
    "\n",
    "4.\tEvaluation:\n",
    "\n",
    "    •\tModel performance was assessed based on its ability to generate task-specific responses, measured via qualitative (human feedback) examination of outputs.\n",
    "\n",
    "### Results\n",
    "\n",
    "Initial Configuration: The model trained with epochs=5 produced responses that lacked coherence and alignment with the prompts, indicating insufficient training time.\n",
    "\n",
    "Improved Configuration: Increasing the training duration to epochs=10 and  virtual_tokens=20 led to significantly better results. The model generated contextually relevant and fluent responses, adhering more closely to the task requirements.\n",
    "\n",
    "Training Time: Despite the increased epochs and number of virtual tokens, the training remained computationally efficient.\n",
    "\n",
    "\n"
   ],
   "id": "dd0187867eb8a79e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:20:00.222176Z",
     "start_time": "2025-01-12T06:19:55.746271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "id": "555dc7c968617173",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanluk/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/ivanluk/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:30:17.083810Z",
     "start_time": "2025-01-12T06:30:15.361935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = tokenizer(\"I want you to act as a rapper.\", return_tensors='pt')\n",
    "output = tokenizer.batch_decode(\n",
    "    model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=100,\n",
    "        repetition_penalty=1.5,\n",
    "        eos_token_id=tokenizer.eos_token_ids\n",
    "    ), skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(\"Original model:\\n\", output)"
   ],
   "id": "bd3ffbea47c96890",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Original model:\n",
      " ['I want you to act as a rapper. I\\'m not gonna be the one of them.\"\\n\"You\\'re going through this?\" he asked, \"and it\\'s like being in an old times? You know what that means when they say \\'you are?\\' and then there is no way for me or anybody else who knows anything about my life right now because we don\\'t have any records at all!\" He paused again before continuing: \"\\'Cause if people think your record label doesn\\'t care']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The model is not quite sure of the context.",
   "id": "2016b673c04f11fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here, I am using the `fka/awesome-chatgpt-prompts` dataset from HuggingFace for tuning the model. This dataset provides motivational content for tuning, ensuring that the model adapts its responses accordingly.",
   "id": "b136b7921efb8097"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:20:16.580041Z",
     "start_time": "2025-01-12T06:20:10.124844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_path = \"fka/awesome-chatgpt-prompts\"\n",
    "dataset = load_dataset(dataset_path)\n",
    "dataset = dataset.map(lambda x: tokenizer(x['prompt']), batched=True)\n",
    "train_prompt = dataset[\"train\"].select(range(203))"
   ],
   "id": "d07ac4f79d632612",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:22:59.027Z",
     "start_time": "2025-01-12T06:22:58.968825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
    "\n",
    "tuning_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.RANDOM,\n",
    "    num_virtual_tokens=5,\n",
    "    tokenizer_name_or_path=model_name\n",
    ")\n",
    "\n",
    "pt_model_with_5_vt = get_peft_model(model, tuning_config)"
   ],
   "id": "c6ecbec97e1bf065",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:23:04.732474Z",
     "start_time": "2025-01-12T06:23:04.709171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    use_cpu=True,\n",
    "    output_dir=\"./\",\n",
    "    auto_find_batch_size=True,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=0.05,\n",
    "    optim='adamw_hf',\n",
    ")"
   ],
   "id": "16fc033d5a37818a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:23:11.258917Z",
     "start_time": "2025-01-12T06:23:11.201628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=pt_model_with_5_vt,\n",
    "    args=training_args,\n",
    "    train_dataset=train_prompt,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ],
   "id": "1f509dbb452ab9f7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:27:59.304874Z",
     "start_time": "2025-01-12T06:23:12.974744Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "c2de19dad77673bc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanluk/Library/Python/3.9/lib/python/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 04:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=130, training_loss=3.859867976262019, metrics={'train_runtime': 286.1026, 'train_samples_per_second': 3.548, 'train_steps_per_second': 0.454, 'total_flos': 92923509888000.0, 'train_loss': 3.859867976262019, 'epoch': 5.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:29:28.982406Z",
     "start_time": "2025-01-12T06:29:26.426983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = tokenizer(\"I want you to act as a rapper.\", return_tensors='pt')\n",
    "\n",
    "original_model_output = tokenizer.batch_decode(\n",
    "    model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=100,\n",
    "        repetition_penalty=1.5,\n",
    "        eos_token_id=tokenizer.eos_token_ids\n",
    "    ), skip_special_tokens=True\n",
    ")\n",
    "\n",
    "pt_model_with_5_vt_output = tokenizer.batch_decode(\n",
    "    pt_model_with_5_vt.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=100,\n",
    "        repetition_penalty=1.5,\n",
    "        eos_token_id=tokenizer.eos_token_ids\n",
    "    ), skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_output}')\n",
    "print(dash_line)\n",
    "print(f'PROMPT TUNED (5 VIRTUAL TOKENS) MODEL:\\n{pt_model_with_5_vt_output}')"
   ],
   "id": "b897cc6b1a32c6a3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/Users/ivanluk/Library/Python/3.9/lib/python/site-packages/peft/peft_model.py:1889: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "['I want you to act as a rapper. I\\'m not going into that.\"\\n\"You\\'re gonna be like, \\'Oh my God.\\' \" —Drake on his new album The Black Album (featuring Drake)\\n\\n']\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PROMPT TUNED (5 VIRTUAL TOKENS) MODEL:\n",
      "['I want you to act as a rapper.\\nThe first thing I wanted is to be able for the audience to understand what it means and how they can relate with each other, so that we are all have an understanding of our own personalities.\"\\n\\n\\xa0This was my second attempt at this project: \"A New York City\" (the last one being about 20 years ago). It\\'s been going on since 2005 when there were no more than 10 people in NYC who could sing along or even listen']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, the model output has improved and the training cost (time and computation) is relatively low, so I decided to increase the training epoches and number of virtual tokens.",
   "id": "f761d85e7e76f974"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:32:49.346834Z",
     "start_time": "2025-01-12T06:32:49.340840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tuning_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.RANDOM,\n",
    "    num_virtual_tokens=20,\n",
    "    tokenizer_name_or_path=model_name\n",
    ")\n",
    "\n",
    "pt_model_with_20_vt = get_peft_model(model, tuning_config)"
   ],
   "id": "308e5c6d664700",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:32:53.370683Z",
     "start_time": "2025-01-12T06:32:53.364971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    use_cpu=True,\n",
    "    output_dir=\"./\",\n",
    "    auto_find_batch_size=True,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=0.05,\n",
    "    optim='adamw_hf',\n",
    ")"
   ],
   "id": "bfb70f6a46643a04",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:32:55.424160Z",
     "start_time": "2025-01-12T06:32:55.411751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=pt_model_with_20_vt,\n",
    "    args=training_args,\n",
    "    train_dataset=train_prompt,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ],
   "id": "892889b1defa2fd8",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:43:08.862131Z",
     "start_time": "2025-01-12T06:32:58.886562Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "1c6140817163d8e9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanluk/Library/Python/3.9/lib/python/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='260' max='260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [260/260 10:08, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=260, training_loss=3.6462167593149037, metrics={'train_runtime': 609.754, 'train_samples_per_second': 3.329, 'train_steps_per_second': 0.426, 'total_flos': 185634720000000.0, 'train_loss': 3.6462167593149037, 'epoch': 10.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:44:29.906053Z",
     "start_time": "2025-01-12T06:44:25.304513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = tokenizer(\"I want you to act as a rapper.\", return_tensors='pt')\n",
    "\n",
    "original_model_output = tokenizer.batch_decode(\n",
    "    model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=100,\n",
    "        repetition_penalty=1.5,\n",
    "        eos_token_id=tokenizer.eos_token_ids\n",
    "    ), skip_special_tokens=True\n",
    ")\n",
    "\n",
    "pt_model_with_5_vt_output = tokenizer.batch_decode(\n",
    "    pt_model_with_5_vt.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=100,\n",
    "        repetition_penalty=1.5,\n",
    "        eos_token_id=tokenizer.eos_token_ids\n",
    "    ), skip_special_tokens=True\n",
    ")\n",
    "\n",
    "pt_model_with_20_vt_output = tokenizer.batch_decode(\n",
    "    pt_model_with_20_vt.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=100,\n",
    "        repetition_penalty=1.5,\n",
    "        eos_token_id=tokenizer.eos_token_ids\n",
    "    ), skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_output}')\n",
    "print(dash_line)\n",
    "print(f'PROMPT TUNED (5 VIRTUAL TOKENS) MODEL:\\n{pt_model_with_5_vt_output}')\n",
    "print(dash_line)\n",
    "print(f'PROMPT TUNED (20 VIRTUAL TOKENS) MODEL:\\n{pt_model_with_20_vt_output}')"
   ],
   "id": "360c66f580c09900",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "['I want you to act as a rapper. I\\'m not saying that\\'s what he does, but it is the way we do things like this year and how they\\'re doing them in their own ways.\"\\n\"It was just one of those moments where there were two or three different guys who had been on stage for years with me being so much more than myself,\" said Taylor Swift at his first performance last week before an audience during The New York Times Music Awards press conference Thursday night (Feb 9']\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PROMPT TUNED (5 VIRTUAL TOKENS) MODEL:\n",
      "['I want you to act as a rapper.\\nThe following are some of the most important things that I\\'ve learned about music in my life:\\n\\n\"You can\\'t be an artist without being able-bodied.\" - The Beatles\\' \"A Day In A New York City\". (1954)']\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PROMPT TUNED (20 VIRTUAL TOKENS) MODEL:\n",
      "['I want you to act as a rapper. I will play the role of an artist, and your actions are not limited by any particular genre or style; they can be anything from hip-hop music (elevine), rock \\'n\\' rollers/rock nazisaws (\"The Rock N Roll\"), jazz ballads / raves,\" etc., but also include some other genres such \"dance\"/\"pop.\" You may use words like rap lyrics in order for example:\\nR']\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see now, the model knows what the rapper does.\n",
    "\n",
    "The results of this experiment demonstrate the importance of tuning hyperparameters, particularly the number of training epochs, in achieving optimal performance with prompt tuning. By increasing the epochs to 10 and virtual tokens to 20, the model achieved significant improvements without compromising on efficiency, highlighting prompt tuning’s potential as an effective and resource-efficient approach for task adaptation in large language models."
   ],
   "id": "355f9174e7e99970"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "565856d3442dd36e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
